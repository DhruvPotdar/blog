[{
    "title": "Behaviour Based Robotics",
    "date": "",
    "description": "This article talks about the basics of behaviour based robotics and shows some examples",
    "body": "Have you ever wondered why we humans survived the race of evolution amongst all the other creatures that inhabited Earth millions of years ago?\nIt is because of our intelligence and ability to acclimatize to a changing environment. Similarly, the idea of robots operating in response to their environment was proposed in the late eighties. This was known as Behavior-Oriented Artificial Intelligence.\nIn that era, robots were designed to do labor-intensive tasks and the term artificial intelligence was limited to robots executing specific sets of instructions on their own which were already fed to their system at the time of design which was previously known as ‘classical AI’. This common approach towards robotics changed over course of time when the need for robots started to arise not only to do labor-intensive tasks but to also perform complex tasks where humans cannot reach, for example exploring the depth of Earth or cleaning difficult to reach sewage blocks. This led to the current development in the definition of AI and furthered the idea of Behavior-Based Robotics.\nBehaviour-Based Robotics, or BBR can be defined as an approach in robotics that focuses on robots that can exhibit complex appearing behaviors despite little internal variable states to model its immediate environment and gradually correct its actions via sensory-motor links. This definition might seem daunting at the beginning but it is quite straightforward to understand and is explained further.\nBehavior-based robots respond to their external stimuli which enables them to avoid obstacles and complete their tasks. For example, if a robot is supposed to deliver some instrument in a particular hospital room. Mapping the whole structure of a hospital is not beneficial as it will keep changing according to real-time situations and there might be too much crowd for the robot to move in a previously empty room, that is why mapping a small area in its environment enables the robot to work efficiently and also enables the robot to have less complex instruction set as it has to only implement actions like wandering, obstacle avoiding and delivering the package while classical AI-Based robot will have to include mapped environment of the hospital and have to find the best path according to the map which might not work in a real-time scenario.\n Classical AI Approach\n  The figure above illustrates the classical AI approach which focuses on planning out the course of actions beforehand and then doing the actual task while the figure below illustrates a behavior-based approach that is doing the basic actions like wandering, obstacle avoiding, etc. by actuators from information sensed from its external environment.\n Behaviour Based Robot for Obstcale Avoidance\n  This huge advantage of adaptability and less complexity has made BBR popular. Some features of behavior-based robots are mentioned below:\n Autonomous: behavior-based robots are highly autonomous, that is they are not dependent much on human instructions for executing their task. Mechanically Imprecise: Classic AI-based robots need to be mechanically precisely shaped as all instructions which are to be executed are given to robots taking into account their precise shape. Behavior-based Robots do not require precise shape since they are adaptable to their environment. This enables us to design robots using BBR at a cheaper cost and less weight. Less Computational Resources: Computational Resources are less in robots designed using BBR as discussed in the example at the beginning of the article. Improving through Learning: Robots designed using BBR can improve through learning which enables them to cope up with the surprises they might face in their tasks. Software can be Reused: Behavior-based Robots rely more on types of sensors and motors connected to them than a concrete structure which enables us to port low-level code from one robot to another. Integrated into an Environment: Behavior-based robots are not seen as isolated devices but as a part of their environment due to their adaptability. Behavior-based Robots tend to be more natural in their behaviors (i.e., their movements). Just like an animal behaves as an integrated part of its surroundings, behavior-based robots behave as a part of their integrated environment.  Along with these features, Behavior-based Robots need to prioritize their tasks to ensure that their survival or the survival of their target is ensured. To achieve this, Behavior-based Robots are first equipped with the most fundamental of all behaviors, that deal with their survival. At work, these robots can prioritize their tasks according to the situations they face. For example, if a robot faces an issue of low battery, then its priority task should be to find a nearby charging station even if it makes it move apart from its main goal as the robot should remain functional throughout the task to complete it. In some scenarios, a robot has to sacrifice itself to protect a human being in case of bomb-defusing robots or if it can cause damage to some other things like important artificial satellites while exploring space.\nBehavior-based Robotics has its key feature adaptation which we have observed from a variety of living species around us for many generations. That is why designs of Behavior-based Robots are inspired by biological structures of different species(biologically inspired) or their behaviors(ethologically inspired). These Behavior-based Robots have a wide range of applications that are used in practical applications as well as in futuristic exploratory endeavors. Some of the standard examples of Behavior-based Robotics are Obstacle avoidance, navigation, terrain mapping, chasing/pursuit, object manipulation, task division, and cooperation. Even the Mars rover ‘Sojourner’ is designed to autonomously explore the surface of Mars and analyze its environment. The current advancement in Behavior-based robotics has enabled us to have the following robots:\n Robots inspired by observing equivalent behavior of ants, chimps, chickens, etc. But not physically structured like those species(ethologically inspired).\n\r \r Biologically inspired robots with a structure resembling fish for aquatic locomotion and a structure resembling a bat for terrestrial locomotion.\n\r \r Snake-bot for limbless locomotion and six-legged hexapod robot inspired from cockroach locomotion.\n\r Behavior-based Mars Rover ‘Sojourner’ Currently, research is going on to develop fully autonomous robots and extend the intelligence of these robots to the intelligence of the human brain.\n\r Nasa aims to develop such robots having human-like intelligence for futuristic space missions\nSome disadvantages or inefficiencies associated with BBR:\n Despite having research going on in the field of BBR, the robots made using this technology are showing intelligence not more than insect level intelligence which is not even close to the intelligence and complexity of the human brain. Classical AI-Based robots are more efficient than the BBR in an environment which doesn’t change in real-time or in case tasks to be performed are repetitive like robots used in the manufacturing industry where the same task is to be repeated daily. In these cases, it is convenient to feed all the information to the robot beforehand which can help to perform the tasks easily.  Conclusion Behavior-based Robotics is a rapidly growing field because of its special feature of adaptability. Behavior-Based Robots can be used to perform tasks that human beings can’t like exploring space or analyzing the surfaces of planets in our solar system. This field has immense potential and breakthroughs will be witnessed soon via various research projects that are underway.\nReferences If you want to explore the concept of Behavior-Based Robotics in more detail and technical terms, you can refer to these links or textbooks\n  Arkin Ronald C. “Behavior Based Robotics”. The MIT Press Cambridge, Massachusetts,1998.\n  Andreas Berk. “Behavior-based Robotics, its scope, and its prospects”. Vrije Universiteit Brussel, Artificial Intelligence Laboratory Pleinlaan 2, 1050 Brussels, Belgium\n  Behavioral Robotics Reading\n  Mataric Maja J. “Behavior-based robotics as a tool for synthesis of artificial behavior and analysis of natural behavior”. Trends in Cognitive Sciences – Vol. 2, No. 3, March 1998\n  Bio-inspired Robotics\n  NASA gives MIT a humanoid robot to develop software for future space missions\n  ",
    "ref": "/blog/blog/bbr/"
  },{
    "title": "Model Predictive Control",
    "date": "",
    "description": "The article talks about Model Predictive Control and its numerous application in industries and robotics and describes a few variants of Model Predictive Control developed recently.",
    "body": "The below animation depicts a Model Predictive Control on a dynamic four-legged robot by Yanran Ding.\n RF-MPC on a Qadruped\n  Over the years, the advancement in robotics and related areas has allowed robot models to have complex and non-linear dynamic equations. The application of such robots involves dynamic environments or environments which are highly inaccessible to humans. Such robot models introduced new challenges in the control strategies as typical industrial controllers such as PID controllers can fail in guaranteeing many features in these areas. These new challenges lead to extensive research to develop optimal control strategies to satisfy the needs of different applications. One such strategy developed by researchers is Model Predictive Control (MPC).\n The basic concept of Model Predictive Control as a model-based and optimization-based solution.\n  One of the first-ever applications of MPC was in chemical plants to control the transients of dynamic systems with hundreds of inputs and outputs, subject to constraint. But MPC has come a long way since then, with its application covering many fields and significant improvement in its technical capabilities.\nImagine walking in a dark room; we first try to sense our surroundings, then based on that, we try to predict the best path towards our goal, and finally, we take only one step on that path and then repeat this entire cycle. MPC also works on a very similar line of reasoning. The essence of MPC is to optimize the manipulatable inputs and the forecasts of process behavior. If we have a reasonably accurate dynamic model of a system, we can use this model and current measurements to predict the future outputs of the system. We can then make the appropriate changes in the input variable based on these predictions.\n Block diagram of model predictive control\n  At each timestep, MPC solves an open-loop optimization problem for the prediction horizon to compute control. The prediction horizon decides how far in the future should the model predict the state of the system. The sequence of control moves calculated by the model corresponding to these predictions is called the control horizon. Even though a sequence of multiple control moves is calculated at each sampling instant, only the first move is implemented on the system. This is the reason why MPC is also called receding horizon control. After this, a new sequence is calculated for the next sampling instant, after the latest measurements become available, and again only the first input move is implemented.\nOne obvious question that arises in mind is why we are calculating a sequence of multiple inputs if we are applying only the first input? A significant benefit of MPC arises from determining the optimal operating conditions (setpoints) and moving the process to these set points in an optimal manner based on the control calculations. By applying only the first input and recalculating the control moves based on new predictions, we ensure that all inputs are based on the optimal operating conditions, irrespective of how our system parameters are changing with time.\n Illustration of the basic concept of model predictive control\n  MPC relies on the provided model for its computations. The model selection has a significant role in the algorithm\u0026rsquo;s computational complexity, like its theoretical properties (e.g. stability). At the same time, the selected objective and imposed constraints also influence and define these properties.\nAnother critical benefit of MPC is its ability to handle inequality constraints. Inequality constraints were a primary motivation for the early development of MPC. Input constraints occur due to physical limitations on the system such as motors, pumps, etc.\nMPC can explicitly handle constraints on both input and output because of its approach of solving open-loop optimization problems for the cost function, which can be solved easily subjected to given constraints. The nature of the optimization problem in linear MPC is a convex function, i.e. the polygon area over which optimization occurs are convex polygons. These problems are commonly referred to as Convex Quadratic Program (QP).\n Convex and Non-Convex polygons\n  The optimization problem commonly used in MPC is the finite-time optimal control problem (FTOCP), i.e. the cost function is optimized for a finite horizon by imposing a terminal constraint.\n Cost function optimization subjected to inequality constraints for MPC\n  Some other benefits of MPC include allowing time delays, inverse response, inherent nonlinearities (changing dynamics), and changing control objective and sensor failure because of its predictive nature.\nIf we think about the quadruped simulation (RF-MPC on quadruped) we saw above, we can understand why MPC is a popular choice for such systems. Designing and controlling a legged robot requires the control design to use its inherent dynamics while dealing with constraints due to hardware limitations and interactions with the environment, which are some of the significant benefits of MPC. The movement of legged robots mainly involves controlling the joint angles of the legs. The inputs given to motors present at these joints must be strictly within a specific range as any arbitrary movement of legs can lead to damage to the robot. In legged robots, multiple variables are required to be controlled, such as joint angles, CoM of the robot, the robot\u0026rsquo;s velocity, the velocity of the end effectors etc., which can be handled conveniently by MPC by designing a multi-variable cost function.\nSome of the popular robots that use MPC are ANYmal by ETH Zurich, Atlas by Boston Dynamics and MIT Cheetah.\n\r ANYmal by ETH Zurich\n  Variants of MPC Non-Linear MPC: Nonlinear Model Predictive Control, or NMPC, is a variant of model MPC, characterized by non-linear system models. NMPC also requires the iterative solution of optimal control problems on a finite prediction horizon as in linear MPC. While these problems are convex in linear MPC, in non-linear MPC, they are not necessarily convex anymore.\n Block diagram of non-linear MPC\n  Explicit MPC: Explicit MPC (eMPC) allows fast evaluation of the control law for some systems, in stark contrast to the online MPC. Explicit MPC is based on the parametric programming technique (optimization problem is solved as a function of multiple parameters). The solution to the MPC control problem formulated as an optimization problem is pre-computed offline. This offline solution, i.e. the control law, is often in the form of a piecewise affine function (PWA).\n eMPC of LPV system in controllable canonical form\n  Other than these, there are many more variants of MPC coming up, such as robust MPC and Hybrid MPC, which can account for set bounded disturbance while still ensuring state constraints are met.\nGiven its remarkable success, MPC has been a popular subject for academic and industrial research. Significant extensions of the early MPC methodology have been developed, and theoretical analysis has provided insight into the strengths and weaknesses of MPC. There are many well-documented implementations of MPC available online one of such implementation is by Atsushi Sakai. Matlab also has an inbuilt toolbox specifically dedicated to MPC. To understand more about Model Predictive Control you can also visit the ERC handbook page.\nReferences  Seborg, D. E.; Mellichamp, D. A. \u0026amp; Edgar, T. F. (2011), Process Dynamics and Control, John Wiley \u0026amp; Sons. (Chapter 20th, Model Predictive Control) Ruchika, Neha Raghu.\u0026quot; Model Predictive Control: History and Development\u0026quot;. International Journal of Engineering Trends and Technology (IJETT) What is Model Predictive Control (MPC), Mitesh Agrawal, August 10, 2020 Prof. S. Boyd, EE364b, Stanford University EE392m - Spring 2005, Gorinevsky, Stanford University Y. Ding, A. Pandala and H. -W. Park, \u0026ldquo;Real-time Model Predictive Control for Versatile Dynamic Motions in Quadrupedal Robots,\u0026rdquo; 2019 International Conference on Robotics and Automation (ICRA) Linear Model Predictive Control, Autonomous Robots Lab  ",
    "ref": "/blog/blog/model_predictive_control/"
  },{
    "title": "Heuristically guided Sampling based Path Planning",
    "date": "",
    "description": "The article talks about the problem of path planning and introduces various different existing algorithm.",
    "body": "One of the easiest tasks for us humans to do is move from one place to another without even meticulously planning our way. We get up from our seats and walk to the cafe, get our drink and come back to our seats. During this entire “action” that we performed, we hardly give any thought about the “trajectory” that we have to follow. For example, while walking or running, when we see that there is a banana peel or a pothole in our “path”, we tend to avoid it. This is something that comes to us naturally as humans. But for a robot, this is a complex task. There is a lot of planning required to enact certain trajectories that come naturally to humans.\nLet us understand the planning problem of our robot, which now has to get coffee from a cafe that is at some distance away from its chair. From our robot’s perspective, the “search space” can either be “discrete” or “continuous”. Discrete search space is easier to work with but is not viable as we cannot perfectly discretize a high dimensional search space and the movement of the robot in a discrete space gets tricky. The better option would be to employ algorithms to search a continuous search space and obtain better results. But since the search space is continuous, you will have to “sample” the search space and look for an optimal solution. An optimal solution is the solution having the least cost.\n Discrete search space\n   Continuous search space\n  Now that we have decided that we will be working with continuous search spaces, the most general algorithm to search for a solution is to sample the search space and connect nodes that are “connectable”. Now, this might sound simple, but there are two things that complicate our problem. One, what is the probability that you will find a solution since you are randomly sampling the search space? Two, what are connectable nodes?\nThe first question involves how we sample our search space. Theoretically, the probability that we find a solution goes to one as the number of samples goes to infinity. This makes sense right! As you completely search the space, the more probable you are to find the solution. But the caveat here is that you don\u0026rsquo;t know how fast you will find the solution. In more technical terms, the rate of convergence of the algorithm is not guaranteed. There are many algorithms where once an initial solution is found, no matter the cost of the initial solution, sampling is done “around” the initial path as samples that might improve the solution lie in close proximity to the initial solution.\nLet us take the example of an Ellipsoid sampler as used in the algorithms Informed RRT*, Batch Informed Trees* etc.\n Ellipsoid Sampler\n   Ellipsoid sampler at work\n  The second question involves how we evaluate the cost of our path. Cost function is generally something that tells us “how expensive” is connecting one node to another. Generally, it is hard to compute. This is so because the whole dynamics and kinematics of the robot have to be taken into account, which increases in complexity as the number of dimensions of your search space increases. Hence, we try to estimate our cost and use “heuristics” to guide our search and delay computing our cost as much as possible.\nHeuristic-based search of an implicit Randomly Generated Graph (RGG) is used in Batch Informed Trees* where the actual cost of connecting two nodes is delayed until we are sure that adding the node would lead to a better solution. Instead of the actual cost an estimated cost is used. In addition to the estimated costs, various heuristics such as cost to come, cost to go, cost to come through the tree etc. are used.\n Use of Heuristics\n  Now that we have kind of understood and solved the questions, we can sample our search space and construct an RGG. The RGG will now be searched using our heuristic functions. A Tree from start node to the goal node will be constructed. If adding any node to the tree might improve the solution we do so by either rewiring the tree or deleting some edges and vertices of the tree and adding new ones.\n\r Randomly Generated Graph that has to be searched\n  Once an initial solution has been found, the algorithm will asymptotically converge to the optimal solution, thus enabling our robot to go and get its coffee.\nWhat\u0026rsquo;s Next Now that we have a path from our chair to the coffee table, some questions arise. Is the path smooth for the robot to travel? How does the robot tackle situations where a person comes in between all of a sudden? Could we have improved our search?\nPath smoothening is done keeping in mind the dynamics of the robot, so that the trajectory generated is continuous. Research is being done to incorporate the generation of smooth paths.\nAlgorithms like Dynamic A*(D*) or Anytime Dynamic A* take into consideration the collision data, i.e when the need arises to dynamically change the path, the algorithm uses previously stored data to arrive at a solution much quicker, instead of re-searching the whole space again.\nThe guiding force behind the search of our algorithm are the heuristic functions. But there may come a time, when the heuristic you use hinders the search. Hence we need adaptive heuristics that can adapt to the given problem or space.\n There is always a need for intelligent systems. Data driven approaches to path planning are also being researched upon. Research in this field will continue until the day a robot can fetch a cup of coffee as naturally as we do.\nReferences  Combining the Two-Layers PageRank Approach with the APA Centrality in Networks with Data - Scientific Figure on ResearchGate. Available here [accessed 15 Mar, 2021] Gammell, Jonathan D., Siddhartha S. Srinivasa, and Timothy D. Barfoot. \u0026ldquo;Batch informed trees (BIT*): Sampling-based optimal planning via the heuristically guided search of implicit random geometric graphs.\u0026rdquo; 2015 IEEE international conference on robotics and automation (ICRA). IEEE, 2015. Strub, Marlin P., and Jonathan D. Gammell. \u0026ldquo;Adaptively Informed Trees (AIT*): Fast asymptotically optimal path planning through adaptive heuristics.\u0026rdquo; 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020. Gammell, Jonathan D., and Marlin P. Strub. \u0026ldquo;A survey of asymptotically optimal sampling-based motion planning methods.\u0026rdquo; arXiv preprint arXiv:2009.10484 (2020). Gammell, Jonathan D., Siddhartha S. Srinivasa, and Timothy D. Barfoot. \u0026ldquo;Informed RRT*: Optimal sampling-based path planning focused via direct sampling of an admissible ellipsoidal heuristic.\u0026rdquo; 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2014.  ",
    "ref": "/blog/blog/heuristically_guided_sampling_based_path_planning/"
  },{
    "title": "Semantic Scene Understanding in Robotics",
    "date": "",
    "description": "An article on why semantic scene understanding could be the next big thing in autonomous robotics, where we are now, and what comes next.",
    "body": "I accidentally dropped my keys this morning. They fell and slid down right under the bed. I got down, making sure not to knock anything over, and tried to reach them, but they seemed out of reach. I instinctively looked around to see if I could use anything to reach the keys, and saw an unused guitar-stand nearby. It took me 4-5 seconds to dismantle the stand, and use its rod to retrieve my keys. This whole \u0026lsquo;side-quest\u0026rsquo; barely cost me 25 seconds.\nBeing a human being, retrieving a key from under a bed is a task that is too insignificant for us to ponder over. We overcome multiple such hinderances daily, without realizing their complexity. For today\u0026rsquo;s robots (even the most advanced ones), this very task would be extremely complex. Why is that so?\nRobotic arms and grippers of our age possess amazing dexterity and recent robots have overcome some of the hardest challenges in control. With advanced sensors, processors and algorithms, tasks such as localization \u0026amp; motion planning are more efficient than ever before. Developments in AI have made real-time object detection and tracking possible. Yet, tasks such as the one mentioned above still seem near-impossible due to a simple unanswered question: How would a robot figure out what to do in such unplanned situations?\nDespite employing state-of-the-art neural networks, robots do not possess a true understanding of the semantics of their environment, i.e. the meaning of the things around them. Robots do not know how every object in their surroundings can influence their goal. Complex robot use cases (eg: disaster management and search \u0026amp; rescue operations) that require robots to predict possible scenarios and anticipate consequences of their actions. But they are unable to correlate knowledge from different domains with what they perceive and therefore, cannot draw logical conclusions about what can/should be done. Thus, the large gap between perception and planning is that of reasoning or common sense.\nSemantic SLAM One of the relatively more explored sub-fields of semantic scene understanding is semantic SLAM (Simultaneous Localization and Mapping). Most techniques in semantic SLAM simply augment a map with semantic information. In earlier works, keypoints extracted within object detection bounding boxes are used to introduce physical constraints during SLAM. Knowledge of recognized objects can be used to introduce semantic priors (eg: A tree should be vertically oriented, the trunk should always touch the ground). Many approaches also predict 3D structures that are not fully visible, by using known shapes of common objects. This helps the robot predict occluded structures and free-spaces. Recent approaches go further by taking into account factors such as the movability, flexibility, allowed degrees of freedom and expected behaviour of known objects. Applications of these are commonly seen in self-driving cars. Here, objects like road signs, cars, signals, etc. are tracked, and known behaviours of vehicles, pedestrians, etc. are used for making predictions.\n  Semantic SLAM - How Tesla Autopilot sees the world (left) and augmenting SLAM with object information (right)\n  Inclusion of properties of individual objects in SLAM is not sufficient. For a true semantic understanding, the robot must also understand how multiple entities interact with one another. This implies that the perceived data must be further augmented with context and reasoning.\nOntology and Logic For robots to make sense of the inter-relations of objects, our knowledge of these objects needs to be arranged in some form of conceptual hierarchy. A common approach towards this is using conceptual maps. These maps are an abstraction of the environment in terms of a graph. In some approaches, nodes represent physical area-labels (room, corridor) and their transition points (doors, gates). Other approaches further cluster objects based on which node they are associated with (eg: An oven is associated with a kitchen). Often, Machine Learning is used for this classification, i.e. associating places \u0026amp; objects to each other, through context. The maps are sometimes endowed with additional semantic constraints such as connectivity, movability, transition feasibility, etc.\nMoving beyond simply recognizing context, robots also need to know how to use this context. An interesting way to tackle this is through the use of description sections. These are sections in the robot\u0026rsquo;s memory that would contain rules for logical inference, Bayesian predictions, heuristics, etc. There also exist algorithms that convert linear \u0026amp; temporal logic from these descriptive rules to controllers that can directly be used on robots.\nAnother effective way to introduce logical reasoning in robots is by storing knowledge in an ontological structure. This is a graph that provides the robot information about what the objects are, what they can be used for and how to use them (See the figure below). Thus, by taking into account the semantics of objects, the robot can choose appropriate behaviour based on the current situation.\n Ontological structures - Cropped diagram of a robot figuring out how to find a cup of tea\n  Learning to Reason Hardcoding logical rules still has its limitations. Human understanding is beyond a fixed set of pre-fed rules. Humans can observe their surroundings and instantly understand what is going on through common sense. Humans can corelate past observations to gain a better understanding of the current situation.\nOne area where machines are getting better at gaining such an understanding is not in robotics, but in artificial video/image captioning networks. The figure below shows an AI answering questions about an image. Such architectures generally consist of a CNN-based (Convolutional Neural Network) model for object recognition giving a feature vector. This feature vector is fed to an RNN (Recurrent Neural Network) that generates a sentence describing it. It would thus be a fair assumption that at an intermediate step of this process contains a significant level of semantic understanding, embedded in latent space.\nThe second area of learning through observation is imitation learning. There exists research in this area that tries to solve the correspondence problem; i.e.: Humans and robots perceive and interact with the world in fundamentally different ways, so robots must learn the correspondence between the \u0026lsquo;state spaces\u0026rsquo; of humans and robots. This consists of establishing Perceptual equivalence (What does an observation mean in robot-terms \u0026amp; human-terms) and physical equivalence (How to achieve the same effect as the one observed).\n General Intelligence \u0026amp; Artificial Conscience Despite all these advances, there is still a very long way to go. As we build systems with growing semantic understanding, we gradually approach towards Artificial General Intelligence (AGI). AGI can be defined as the ability of a machine to perform any task that a human can. Contemporary state-of-the-art systems are still designed to perform well on very specific tasks, but not so much on anything else. An AGI on the other hand should be able to learn a broader range of tasks with far less training. An AGI would ideally be able to apply knowledge of one domain to another.\nAn AGI singularity is defined as the point in the future when Artificial Intelligence surpasses human level thinking. Based on current trends of advancement in the field, some experts believe that the singularity may arrive as early as the year 2060. As each development in robotics and AI brings us closer to this point, we are moving slowly from \u0026lsquo;Autonomous Robots\u0026rsquo; to 'Cognitive Robots' - robots that possess awareness, memory (episodic \u0026amp; procedural), ability to learn and the ability to anticipate. At such a point, AI may have the ability to figure out solutions to some of the worlds biggest and most complex challenges, and robots may be able to implement these solutions with far more ease and efficiency than humans. This would be a point when a simple task like figuring out how to retrieve keys from under a bed would truly be as insignificant for robots as it is for humans. But until then, there still remains much research to be done.\nReferences  R.Salas, N.Newcombe, H.Strasdat, P.Kelly, A.Davidson; SLAM++: Simultaneous localization and mapping at the level of Objects; CVPR 2013 I.Kostavelis, A.Gasteratos; Semantic Mapping for Mobile Robot Tasks - A survey; Robotics \u0026amp; Autonomous Systems S66 (2015) pp86-103 This is what a Tesla Autopilot sees on Road (Carscoop Article by S.Tudose) R. Zellers, Y.Bisk, A.Farhadi, Y.Choi: From Recognition to Cognition - Visual Common Sense Reasoning; CVPR 2019 A. Pronobis, P. Jensfelt, Understanding the real world: Combining objects, appearance, geometry and topology for semantic mapping. G.Lim; Ontology based unified robot knoowledge for Service Robots in Indoor Environment; IEEE transactions on Systems, Man \u0026amp; Cybernetics Part A. Vol 41. No 3, May 2011 C. Galindo, A. Saffiotti, S. Coradeschi, P. Buschka, J.-A. Fernandez-Madrigal, J. González: Multi-hierarchical semantic maps for mobile robotics, International Conference on Intelligent Robots and Systems, IEEE, 2005, pp. 2278–2283 O.M. Mozos, W. Burgard, Supervised learning of topological maps using se- mantic information extracted from range data; International Conference on Intelligent Robots and Systems, IEEE, 2006, pp. 2772–2777 B. Kuipers, Modeling spatial knowledge, Cogn. Sci. 2 (2) (1978) 129–153. Automatic Image Captioning using Deep Learning (Medium Article) J. Browniee, 2017 How to Automatically Generate Textual Descriptions for a Photograph with DL S. Wadhwa, 2018 Asking Questions to Images with Deep Learning (Floydhub Article) M. Tenorth, L. Kunze, D. Jain, M. Beetz, Knowrob-map-knowledge-linked semantic object maps; International Conference on Humanoid Robots, IEEE, 2010, pp. 430–435 L.Nicholson, M.Milford, N.Sünderhauf; QuadricSLAM: Dual Quadrics from Object Detections as Landmarks in Object-oriented SLAM Robot Learning by Demonstration (Scholarpedia) How Far are we from achieving Artificial General Intelligence? (Forbes Article) 995 experts opinion: AGI singularity by 2060 Cognitive Robotics (Wikipedia)  ",
    "ref": "/blog/blog/semantic_scene/"
  },{
    "title": "Robotic Arms: A Brief",
    "date": "",
    "description": "This article gives a brief history of Robotic Arms and discusses their different types and various uses.",
    "body": "Most of the world\u0026rsquo;s robots are designed for hard, repetitive production work. They perform activities that human beings find complicated, dangerous or tedious. The most common production robot is a robotic arm.\nA robotic arm is a device that is programmed to carry out a particular task with extreme accuracy at a rapid pace with great efficiency.\nHistory of the arm The idea of a robot arm is not new. The first robotic arm was designed by Leonardo da Vinci in the late fifteenth century. While analyzing his papers in the 1950s researchers discovered that he had sketched a robotic arm and humanoid figurines which could run on the clockwork technologies available at that time. It used pulleys, weights and gears to provide a partially autonomous motion. This design was assembled in 2002 also termed as the robotic knight which could walk and wave without manual intervention. The arm had 4 degrees of freedom and an analog onboard controller for supplying power.\nIn 1941, Isaac Asimov published a short science fiction story in the magazine where he introduced the Three Laws of Robotics thereby coining the term robotics. He later published the book “I, Robot” in which the characters obeyed the Three Laws of Robotics. This inspired engineer Joseph Engelberger and inventor George Devol, who filed for a patent for a programmed article transfer device — the first version of the robotic arm. In 1961, they started Unimation Inc. which focused on the manufacture of industrial robots. Their flagship was the Unimate 1900- a simple robotic arm.\n Unimate Robot\n  In 1963, researchers at the Rancho Los Amigos Hospital in California developed the Rancho Arm to help move disabled patients. It was the first computer-controlled robotic arm and was equipped with six joints to let it move like a human arm. Engelberg would further travel to Japan to partner with Kawasaki Aircraft to manufacture and sell Unimation’s robots in Japan.\n The Rancho Arm\n  In 1968, on the same principles as that of the Rancho Arm, Marvin Minsky developed the Minsky Arm which had 12 joints which could be controlled by a joystick. The arm was powered by hydraulic fluids. It was used for gentle lifting of the patients.\nIn 1969 Scheinman’s Stanford Arm robot achieved a milestone as the first successful electrically driven, computer- controlled robot arm. By 1974 it was able to guide itself through optical and contact sensors. It was the first arm to provide tactile feedback to its operator.\n The Stanford Arm\n  In 1973, a German company Kuka launched Famulus - a robotic arm which worked by using six electromagnetic axles. This was revolutionary.\nThe CMU Direct - drive Arm I was built in 1981 at the CMU Robotics Institute. This arm had motors installed directly into each joint. The need for chains or tendons used in previous arms was removed by the electric motors housed within the joints. DD arm was faster and more accurate than its counterparts at that time.\n CMU Direct Drive Arm I\n  Currently every automobile manufacturer in the world employs robotic arms in factories for different operations.\nSome examples of modern-day industrial robot arms include the UR series by Universal Robots and the Kuka arms by Kuka robotics.\n The UR5 Robot Arm\n  Construction of a robotic arm The entire system of a robotic arm is based on two elements: the mechanical component and the signal processing component. The signal processing component processes the computational language which is uploaded on the processing unit whereas the mechanical portion is the design of the functioning arm using mechanics.\nAn industrial robotic arm consists of a series of joints, articulations and manipulators that work together to closely resemble the motion and functionality of a human arm. Depending upon the specific task expected by the robotic arm to perform it can have varying number of degrees of freedom i.e., different modes in which it can move. Generally, the motion of a robotic arm is determined under three categories: Roll, Pitch and Yaw.\n Roll, Pitch and Yaw\n  A human hand has pitch and roll along the shoulder; pitch and yaw along the arm and roll, pitch and yaw along the wrist giving a total of 7 degrees of freedom. The wrist or hand part of the robotic arm is also called the end effector.\nA large portion of smaller robotic arms which are used in industries are benchtop mounted and electronically controlled. The larger versions might be floor mounted. They are generally manufactured using sturdy and durable metal like steel and cast – iron. These arms have 4-6 articulating joints used to replicate the human equivalents like wrist, shoulder, elbow, and forearm.\n Types of Robotic Arm There are various distinct robotic arms available today each manufactured for a particular role with special abilities suited for that environment. A large chunk of arms has six joints connecting seven sections but the key distinction lies in the manner in which the joints are designed to articulate and the range of movements they can perform. Some major types of robotic arms are:\nCartesian or gantry robotic arms: These arms consist of three articulating joints that are coincident with the cartesian axes X, Y, and Z. The directions are given in the form of cartesian coordinates to specify movement in three dimensions. Additionally, there may be a wrist joint which provides rotational motion. They are used in picking and placing objects and assembly operations.\nCylindrical robotic arm: This is the arm whose axes form a cylindrical coordinate system. They are used for handling machine tools, assembly operations and spot welding.\nSpherical or Polar robotic arm: This is the arm whose axes form a spherical coordinate system. It is mainly used in die casting, welding, and fettling machines.\nSCARA robotic arm: The term SCARA stands for Selective Compliance Assembly Robot Arm or Selective Compliance Articulated Robot Arm. To provide enforcement in a plane, this arm features two parallel rotary joints.\n SCARA by KUKA\n  Articulated robotic arm: These are robotic arms having at least three rotary joints. They are typically used for assembly operations, die casting, gas welding, arc welding, fettling machines and spray painting.\n Articulated Robot by KUKA\n  Uses of Robotic Arm  GIF from giphy\n  Other than being used as a salt shaker a robotic arm has various applications.\nA robotic arm greatly increases the production rate and accuracy of placement and picking tasks. They are also used for other industrial applications like welding, painting, and drilling. They are also used in servicing of nuclear power plants, thermal power stations and cleaning up radioactive wastes.\nThe arm has applications in space exploration involving the repairs of the space station and collection of samples when mounted on top of a rover.\nBy using sensors and cameras with the arm, they can be used in rescue and relief operations. The assistance of a robotic arm is used in performing surgeries.\nAfter installing barcode scanners on the arm, they are used in inventory management.\nWith the advancing technology, the cost of manufacturing different components of a robot has decreased. This has led to a rapid expansion in the affordability and availability of robots not only for large scale operations but also for small scale operations. With the extensive ongoing research and innovation in areas like computer vision and reinforcement learning, the applications of a robotic arm will continue to increase further. Until then\n GIF from giphy\n  References  Why Was the Robotic Arm Invented? by Megan Ray Nichols Assembling and Controlling a Robotic Arm by Manoel Carlos Ramon Timeline of Computer History  ",
    "ref": "/blog/blog/robotic_arms/"
  },{
    "title": "Modular Self Reconfigurable Robots",
    "date": "",
    "description": "The article discusses the idea of Modular Self Reconfigurable Robots, and how they boost robots' utility in various sectors.",
    "body": "Robots were invented with the goal of helping humans carry out their tasks more comfortably, particularly 4D (Dirty, Dangerous, Difficult, and Dull) tasks. In designing robots, the conventional approach has been to design their hardware and software in accordance with the tasks they are supposed to do. Conventional robots can perform specific tasks accurately, but they are not very versatile and adaptive, and thus applications that are consigned to them rely heavily on their physical structure and controller capabilities. As a workaround to flexibility and adaptability limitations of fixed-body robots, Modular Self Reconfigurable Robots were introduced.\n SMORES by GRASP Lab at Penn Engineering\n  Modular robotics provides a unique advantage over traditional robotic technologies in terms of reconfigurability, reusability, and ease in manufacturing. Many applications such as large-scale facility management, space exploration, military-zone monitoring, disaster management, and prosthetics for physically disabled need adaptable and self-healing capabilities, and modular self reconfigurable robots are often seen as a feasible solution to the same. The major difference of modular system designs over conventional robots can be viewed as the ability to form various configurations as per the requirement of application with minimal human intervention.\nA modular robot consists of several units with few degrees of freedom (DOFs) called modules which are usually equipped with connection mechanisms to cooperatively connect to or detach from each other in order to create complex structures and configurations with many DOFs. Modular robots are usually classified into chain-type, lattice-type or hybrid-type architectures depending on module arrangement.\n M-TRAN by AIST and Tokyo Tech\n  Chain-type architectures consist of modules that are connected together in a linear or tree topology. This structure can fold-up to become space filling, but the underlying architecture is serial. These modular robots are able to autonomously change their configuration into a wheel, quadruped, snake, worm, and so on. Some examples of chain-type modular robots are Polybot, iMOBOT, Transmote, Ubot, CoSMO and many more.\nLattice-type robot has modules that are arranged in a regular three-dimensional (3D) pattern, such as a cubical or hexagonal grid. Lattice-type modular robots are inherently self-reconfigurable because reconfiguration is their only means of locomotion. M-blocks, Telecube, CHOBIE, Atron, Cross-ball are only some of the lattice-type robots.\nHybrid-type architectures have features of both lattice-type and chain-type architectures. Some modular robots can be classified as hybrid-type because they can be configured both as chain and as lattice structures. M-TRAN, Superbot, SMORES, and Roombots are examples of hybrid-type modular robots.\n Atron by Adaptronics group\n  ERC, BITS Goa’s, Modbot project, focused on developing a new design for a modular robot that had sufficient degrees of freedom to be able to perform a large number of configurations. The project aimed at developing a lightweight, manufacturable network of modules able to overcome functional limitations faced by existing modular self reconfigurable robots. A single module was fabricated capable of complete teleoperation. A custom circuit on a prototyping board was manufactured to overcome difficulties in connections and properly interfacing the micro-controller with peripherals. The team also performed torque analysis for gears and linkages in a simulation environment. In Feb 2020, the team began the process of improvising the mechanical model in order to overcome the space constraints to fit in the IR modules (TCRT5000). The future prospects of the project includes migrating the whole project onto ROS and preparing a custom stack for the project.\n Modbot by ERC, BITS Goa\n  A modular robot also contains a lot of electronics on a single module aside from all the mechanical structure discussed here. These range from actuation motors to sensors for perception of the environment and computer processors which can handle all remote computation. To be fully self reconfigurable these robots need to be autonomous and complete various tasks with minimal or no human interference. This requires precise control algorithms, motion and path planning and even the application of machine learning in some stages.\nTherefore the concept of modular robotics has tremendous research potential and a lot more diverse applications. By showing us new ways robots can interact with the real world and adapt to different conditions, they are slowly evolving and changing our idea of robotics. This very well is just the beginning in the world of Modular Self Reconfigurable Robots.\nReferences  Modular robotic systems: Methods and algorithms for abstraction, planning, control, and synchronization by Hossein Ahmadzadeh et.al. Modular Self-Reconfigurable Robotic Systems: A Survey on Hardware Architectures by S. Sankhar Reddy Chennareddy et. al. Current trends in reconfigurable modular robots design by Alberto Brunete et.al.  ",
    "ref": "/blog/blog/mssr/"
  },{
    "title": "About",
    "date": "",
    "description": "",
    "body": "The Electronics and Robotics Club (ERC) of BITS Goa is a diverse group of students with interests ranging from electronics to machine learning to mechanical design. Over the years, we have evolved into a platform to learn and experiment with various aspects of science and engineering and to apply them in robotics.\nThe club includes people who are passionate about the latest technologies and engineering techniques. This blog is an attempt to share their experience with others in specific fields and also to give people a glimpse of trending areas in robotics research. The posts are written by members with a particular interest in the relevant subject matter and may even correspond to a project that the club has worked upon.\nWe are open to anyone with a general interest in engineering and who wants to explore robotics, so feel free to get in touch with us.\nTo know more about us visit our website, and for learning resources to get started on robotics check out our handbook\n",
    "ref": "/blog/about/"
  },{
    "title": "Kontakt",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/contact/"
  }]
