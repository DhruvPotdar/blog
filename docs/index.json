[{
    "title": "Semantic Scene Understanding in Robotics",
    "date": "",
    "description": "An article on why semantic scene understanding could be the next big thing in autonomous robotics, where we are now, and what comes next",
    "body": "I accidentally dropped my keys this morning. They fell and slid down right under the bed. I got down, making sure not to knock anything over, and tried to reach them, but they seemed out of reach. I instinctively looked around to see if I could use anything to reach the keys, and saw an unused guitar-stand nearby. It took me 4-5 seconds to dismantle the stand, and use its rod to retrieve my keys. This whole \u0026lsquo;side-quest\u0026rsquo; barely cost me 25 seconds.\nBeing a human being, retrieving a key from under a bed is a task that is too insignificant for us to ponder over. We overcome multiple such hinderances daily, without realizing their complexity. For today\u0026rsquo;s robots (even the most advanced ones), this very task would be extremely complex. Why is that so?\nRobotic arms and grippers of our age possess amazing dexterity and recent robots have overcome some of the hardest challenges in control. With advanced sensors, processors and algorithms, tasks such as localization \u0026amp; motion planning are more efficient than ever before. Developments in AI have made real-time object detection and tracking possible. Yet, tasks such as the one mentioned above still seem near-impossible due to a simple unanswered question: How would a robot figure out what to do in such unplanned situations?\nDespite employing state-of-the-art neural networks, robots do not possess a true understanding of the semantics of their environment, i.e. the meaning of the things around them. Robots do not know how every object in their surroundings can influence their goal. Complex robot use cases (eg: disaster management and search \u0026amp; rescue operations) that require robots to predict possible scenarios and anticipate consequences of their actions. But they are unable to correlate knowledge from different domains with what they perceive and therefore, cannot draw logical conclusions about what can/should be done. Thus, the large gap between perception and panning is that of reasoning or common sense.\nSemantic SLAM One of the relatively more explored sub-fields of semantic scene understanding is semantic SLAM (Simultaneous Localization and Mapping). Most techniques in semantic SLAM simply augment a map with semantic information. In earlier works, keypoints extracted within object detection bounding boxes are used to introduce physical constraints during SLAM. Knowledge of recognized objects can be used to introduce semantic priors (eg: A tree should be vertically oriented, the trunk should always touch the ground). Many approaches also predict 3D structures that are not fully visible, by using known shapes of common objects. This helps the robot predict occluded structures and free-spaces. Recent approaches go further by taking into account factors such as the movability, flexibility, allowed degrees of freedom and expected behaviour of known objects. Applications of these are commonly seen in self-driving cars. Here, objects like road signs, cars, signals, etc. are tracked, and known behaviours of vehicles, pedestrians, etc. are used for making predictions.\n   Semantic SLAM - How Tesla Autopilot sees the world (left) and augmenting SLAM with object information (right)\n  Inclusion of properties of individual objects in SLAM is not sufficient. For a true semantic understanding, the robot must also understand how multiple entities interact with one another. This implies that the perceived data must be further augmented with context and reasoning.\nOntology and Logic For robots to make sense of the inter-relations of objects, our knowledge of these objects needs to be arranged in some form of conceptual hierarchy. A common approach towards this is using conceptual maps. These maps are an abstraction of the environment in terms of a graph. In some approaches, nodes represent physical area-labels (room, corridor) and their transition points (doors, gates). Other approaches further cluster objects based on which node they are associated with (eg: An oven is associated with a kitchen). Often, Machine Learning is used for this classification, i.e. associating places \u0026amp; objects to each other, through context. The maps are sometimes endowed with additional semantic constraints such as connectivity, movability, transition feasibility, etc.\nMoving beyond simply recognizing context, robots also need to know how to use this context. An interesting way to tackle this is through the use of description sections. These are sections in the robot\u0026rsquo;s memory that would contain rules for logical inference, Bayesian predictions, heuristics, etc. There also exist algorithms that convert linear \u0026amp; temporal logic from these descriptive rules to controllers that can directly be used on robots.\nAnother effective way to introduce logical reasoning in robots is by storing knowledge in an ontological structure. This is a graph that provides the robot information about what the objects are, what they can be used for and how to use them (See the figure below). Thus, by taking into account the semantics of objects, the robot can choose appropriate behaviour based on the current situation.\n  Ontological structures - Cropped diagram of a robot figuring out how to find a cup of tea\n  Learning to Reason Hardcoding logical rules still has its limitations. Human understanding is beyond a fixed set of pre-fed rules. Humans can observe their surroundings and instantly understand what is going on through common sense. Humans can corelate past observations to gain a better understanding of the current situation.\nOne area where machines are getting better at gaining such an understanding is not in robotics, but in artificial video/image captioning networks. The figure below shows an AI answering questions about an image. Such architectures generally consist of a CNN-based (Convolutional Neural Network) model for object recognition giving a feature vector. This feature vector is fed to an RNN (Recurrent Neural Network) that generates a sentence describing it. It would thus be a fair assumption that at an intermediate step of this process contains a significant level of semantic understanding, embedded in latent space.\nThe second area of learning through observation is imitation learning. There exists research in this area that tries to solve the correspondence problem; i.e.: Humans and robots perceive and interact with the world in fundamentally different ways, so robots must learn the correspondence between the \u0026lsquo;state spaces\u0026rsquo; of humans and robots. This consists of establishing Perceptual equivalence (What does an observation mean in robot-terms \u0026amp; human-terms) and physical equivalence (How to achieve the same effect as the one observed).\n  General Intelligence \u0026amp; Artificial Conscience Despite all these advances, there is still a very long way to go. As we build systems with growing semantic understanding, we gradually approach towards Artificial General Intelligence (AGI). AGI can be defined as the ability of a machine to perform any task that a human can. Contemporary state-of-the-art systems are still designed to perform well on very specific tasks, but not so much on anything else. An AGI on the other hand should be able to learn a broader range of tasks with far less training. An AGI would ideally be able to apply knowledge of one domain to another.\nAn AGI singularity** is defined as the point in the future when Artificial Intelligence surpasses human level thinking. Based on current trends of advancement in the field, some experts believe that the singularity may arrive as early as the year 2060. As each development in robotics and AI brings us closer to this point, we are moving slowly from \u0026lsquo;Autonomous Robots\u0026rsquo; to \u0026lsquo;Cognitive Robots\u0026rsquo;** - robots that possess awareness, memory (episodic \u0026amp; procedural), ability to learn and the ability to anticipate. At such a point, AI may have the ability to figure out solutions to some of the worlds biggest and most complex challenges, and robots may be able to implement these solutions with far more ease and efficiency than humans. This would be a point when a simple task like figuring out how to retrieve keys from under a bed would truly be as insignificant for robots as it is for humans. But until then, there still remains much research to be done.\nReferences  R.Salas, N.Newcombe, H.Strasdat, P.Kelly, A.Davidson; SLAM++: Simultaneous localization and mapping at the level of Objects; CVPR 2013 I.Kostavelis, A.Gasteratos; Semantic Mapping for Mobile Robot Tasks - A survey; Robotics \u0026amp; Autonomous Systems S66 (2015) pp86-103 This is what a Tesla Autopilot sees on Road (Carscoop Article by S.Tudose) R. Zellers, Y.Bisk, A.Farhadi, Y.Choi: From Recognition to Cognition - Visual Common Sense Reasoning; CVPR 2019 A. Pronobis, P. Jensfelt, Understanding the real world: Combining objects, appearance, geometry and topology for semantic mapping. G.Lim; Ontology based unified robot knoowledge for Service Robots in Indoor Environment; IEEE transactions on Systems, Man \u0026amp; Cybernetics Part A. Vol 41. No 3, May 2011 C. Galindo, A. Saffiotti, S. Coradeschi, P. Buschka, J.-A. Fernandez-Madrigal, J. González: Multi-hierarchical semantic maps for mobile robotics, International Conference on Intelligent Robots and Systems, IEEE, 2005, pp. 2278–2283 O.M. Mozos, W. Burgard, Supervised learning of topological maps using se- mantic information extracted from range data; International Conference on Intelligent Robots and Systems, IEEE, 2006, pp. 2772–2777 B. Kuipers, Modeling spatial knowledge, Cogn. Sci. 2 (2) (1978) 129–153. Automatic Image Cationing using Deep Learning (Medium Article) J. Browniee, 2017 How to Automatically Generate Textual Descriptions for a Photograph with DL S. Wadhwa, 2018 Asking Questions to Images with Deep Learning (Floydhub Article) M. Tenorth, L. Kunze, D. Jain, M. Beetz, Knowrob-map-knowledge-linked semantic object maps; International Conference on Humanoid Robots, IEEE, 2010, pp. 430–435 L.Nicholson, M.Milford, N.Sünderhauf; QuadricSLAM: Dual Quadrics from Object Detections as Landmarks in Object-oriented SLAM Robot Learning by Demonstration (Scholarpedia) How Far are we from achieving Artificial General Intelligence? (Forbes Article) 995 experts opinion: AGI singularity by 2060 Cognitive Robotcs (Wikipedia)  ",
    "ref": "/blog/blog/semantic_scene/"
  },{
    "title": "Robotic Arms: A Brief",
    "date": "",
    "description": "This article gives a brief history of Robotic Arms and discusses their different types and various uses.",
    "body": "Most of the world\u0026rsquo;s robots are designed for hard, repetitive production work. They perform activities that human beings find complicated, dangerous or tedious. The most common production robot is a robotic arm.\nA robotic arm is a device that is programmed to carry out a particular task with extreme accuracy at a rapid pace with great efficiency.\nHistory of the arm The idea of a robot arm is not new. The first robotic arm was designed by Leonardo da Vinci in the late fifteenth century. While analyzing his papers in the 1950s researchers discovered that he had sketched a robotic arm and humanoid figurines which could run on the clockwork technologies available at that time. It used pulleys, weights and gears to provide a partially autonomous motion. This design was assembled in 2002 also termed as the robotic knight which could walk and wave without manual intervention. The arm had 4 degrees of freedom and an analog onboard controller for supplying power.\nIn 1941, Isaac Asimov published a short science fiction story in the magazine where he introduced the Three Laws of Robotics thereby coining the term robotics. He later published the book “I, Robot” in which the characters obeyed the Three Laws of Robotics. This inspired engineer Joseph Engelberger and inventor George Devol, who filed for a patent for a programmed article transfer device — the first version of the robotic arm. In 1961, they started Unimation Inc. which focused on the manufacture of industrial robots. Their flagship was the Unimate 1900- a simple robotic arm.\n  Unimate Robot\n  In 1963, researchers at the Rancho Los Amigos Hospital in California developed the Rancho Arm to help move disabled patients. It was the first computer-controlled robotic arm and was equipped with six joints to let it move like a human arm. Engelberg would further travel to Japan to partner with Kawasaki Aircraft to manufacture and sell Unimation’s robots in Japan.\n  The Rancho Arm\n  In 1968, on the same principles as that of the Rancho Arm, Marvin Minsky developed the Minsky Arm which had 12 joints which could be controlled by a joystick. The arm was powered by hydraulic fluids. It was used for gentle lifting of the patients.\nIn 1969 Scheinman’s Stanford Arm robot achieved a milestone as the first successful electrically driven, computer- controlled robot arm. By 1974 it was able to guide itself through optical and contact sensors. It was the first arm to provide tactile feedback to its operator.\n  The Stanford Arm\n  In 1973, a German company Kuka launched Famulus - a robotic arm which worked by using six electromagnetic axles. This was revolutionary.\nThe CMU Direct - drive Arm I was built in 1981 at the CMU Robotics Institute. This arm had motors installed directly into each joint. The need for chains or tendons used in previous arms was removed by the electric motors housed within the joints. DD arm was faster and more accurate than its counterparts at that time.\n  CMU Direct Drive Arm I\n  Currently every automobile manufacturer in the world employs robotic arms in factories for different operations.\nSome examples of modern-day industrial robot arms include the UR series by Universal Robots and the Kuka arms by Kuka robotics.\n  The UR5 Robot Arm\n  Construction of a robotic arm The entire system of a robotic arm is based on two elements: the mechanical component and the signal processing component. The signal processing component processes the computational language which is uploaded on the processing unit whereas the mechanical portion is the design of the functioning arm using mechanics.\nAn industrial robotic arm consists of a series of joints, articulations and manipulators that work together to closely resemble the motion and functionality of a human arm. Depending upon the specific task expected by the robotic arm to perform it can have varying number of degrees of freedom i.e., different modes in which it can move. Generally, the motion of a robotic arm is determined under three categories: Roll, Pitch and Yaw.\n  Roll, Pitch and Yaw\n  A human hand has pitch and roll along the shoulder; pitch and yaw along the arm and roll, pitch and yaw along the wrist giving a total of 7 degrees of freedom. The wrist or hand part of the robotic arm is also called the end effector.\nA large portion of smaller robotic arms which are used in industries are benchtop mounted and electronically controlled. The larger versions might be floor mounted. They are generally manufactured using sturdy and durable metal like steel and cast – iron. These arms have 4-6 articulating joints used to replicate the human equivalents like wrist, shoulder, elbow, and forearm.\n  Types of Robotic Arm There are various distinct robotic arms available today each manufactured for a particular role with special abilities suited for that environment. A large chunk of arms has six joints connecting seven sections but the key distinction lies in the manner in which the joints are designed to articulate and the range of movements they can perform. Some major types of robotic arms are:\nCartesian or gantry robotic arms: These arms consist of three articulating joints that are coincident with the cartesian axes X, Y, and Z. The directions are given in the form of cartesian coordinates to specify movement in three dimensions. Additionally, there may be a wrist joint which provides rotational motion. They are used in picking and placing objects and assembly operations.\nCylindrical robotic arm: This is the arm whose axes form a cylindrical coordinate system. They are used for handling machine tools, assembly operations and spot welding.\nSpherical or Polar robotic arm: This is the arm whose axes form a spherical coordinate system. It is mainly used in die casting, welding, and fettling machines.\nSCARA robotic arm: The term SCARA stands for Selective Compliance Assembly Robot Arm or Selective Compliance Articulated Robot Arm. To provide enforcement in a plane, this arm features two parallel rotary joints.\n  SCARA by KUKA\n  Articulated robotic arm: These are robotic arms having at least three rotary joints. They are typically used for assembly operations, die casting, gas welding, arc welding, fettling machines and spray painting.\n  Articulated Robot by KUKA\n  Uses of Robotic Arm   GIF from giphy\n  Other than being used as a salt shaker a robotic arm has various applications.\nA robotic arm greatly increases the production rate and accuracy of placement and picking tasks. They are also used for other industrial applications like welding, painting, and drilling. They are also used in servicing of nuclear power plants, thermal power stations and cleaning up radioactive wastes.\nThe arm has applications in space exploration involving the repairs of the space station and collection of samples when mounted on top of a rover.\nBy using sensors and cameras with the arm, they can be used in rescue and relief operations. The assistance of a robotic arm is used in performing surgeries.\nAfter installing barcode scanners on the arm, they are used in inventory management.\nWith the advancing technology, the cost of manufacturing different components of a robot has decreased. This has led to a rapid expansion in the affordability and availability of robots not only for large scale operations but also for small scale operations. With the extensive ongoing research and innovation in areas like computer vision and reinforcement learning, the applications of a robotic arm will continue to increase further. Until then\n  GIF from giphy\n  References  Why Was the Robotic Arm Invented? by Megan Ray Nichols - https://interestingengineering.com/why-was-the-robotic-arm-invented Assembling and Controlling a Robotic Arm by Manoel Carlos Ramon - https://link.springer.com/chapter/10.1007/978-1-4302-6838-3_11 Timeline of Computer History - https://www.computerhistory.org/timeline/ai-robotics/  ",
    "ref": "/blog/blog/robotic_arms/"
  },{
    "title": "Modular Self Reconfigurable Robots",
    "date": "",
    "description": "The article discusses the idea of Modular Self Reconfigurable Robots, and how they boost robots' utility in various sectors.",
    "body": "Robots were invented with the goal of helping humans carry out their tasks more comfortably, particularly 4D (Dirty, Dangerous, Difficult, and Dull) tasks. In designing robots, the conventional approach has been to design their hardware and software in accordance with the tasks they are supposed to do. Conventional robots can perform specific tasks accurately, but they are not very versatile and adaptive, and thus applications that are consigned to them rely heavily on their physical structure and controller capabilities. As a workaround to flexibility and adaptability limitations of fixed-body robots, Modular Self Reconfigurable Robots were introduced.\n  SMORES by GRASP Lab at Penn Engineering\n  Modular robotics provides a unique advantage over traditional robotic technologies in terms of reconfigurability, reusability, and ease in manufacturing. Many applications such as large-scale facility management, space exploration, military-zone monitoring, disaster management, and prosthetics for physically disabled need adaptable and self-healing capabilities, and modular self reconfigurable robots are often seen as a feasible solution to the same. The major difference of modular system designs over conventional robots can be viewed as the ability to form various configurations as per the requirement of application with minimal human intervention.\nA modular robot consists of several units with few degrees of freedom (DOFs) called modules which are usually equipped with connection mechanisms to cooperatively connect to or detach from each other in order to create complex structures and configurations with many DOFs. Modular robots are usually classified into chain-type, lattice-type or hybrid-type architectures depending on module arrangement.\n  M-TRAN by AIST and Tokyo Tech\n  Chain-type architectures consist of modules that are connected together in a linear or tree topology. This structure can fold-up to become space filling, but the underlying architecture is serial. These modular robots are able to autonomously change their configuration into a wheel, quadruped, snake, worm, and so on. Some examples of chain-type modular robots are Polybot, iMOBOT, Transmote, Ubot, CoSMO and many more.\nLattice-type robot has modules that are arranged in a regular three-dimensional (3D) pattern, such as a cubical or hexagonal grid. Lattice-type modular robots are inherently self-reconfigurable because reconfiguration is their only means of locomotion. M-blocks, Telecube, CHOBIE, Atron, Cross-ball are only some of the lattice-type robots.\nHybrid-type architectures have features of both lattice-type and chain-type architectures. Some modular robots can be classified as hybrid-type because they can be configured both as chain and as lattice structures. M-TRAN, Superbot, SMORES, and Roombots are examples of hybrid-type modular robots.\n  Atron by Adaptronics group\n  ERC, BITS Goa’s, Modbot project, focused on developing a new design for a modular robot that had sufficient degrees of freedom to be able to perform a large number of configurations. The project aimed at developing a lightweight, manufacturable network of modules able to overcome functional limitations faced by existing modular self reconfigurable robots. A single module was fabricated capable of complete teleoperation. A custom circuit on a prototyping board was manufactured to overcome difficulties in connections and properly interfacing the micro-controller with peripherals. The team also performed torque analysis for gears and linkages in a simulation environment. In Feb 2020, the team began the process of improvising the mechanical model in order to overcome the space constraints to fit in the IR modules (TCRT5000). The future prospects of the project includes migrating the whole project onto ROS and preparing a custom stack for the project.\n  Modbot by ERC, BITS Goa\n  A modular robot also contains a lot of electronics on a single module aside from all the mechanical structure discussed here. These range from actuation motors to sensors for perception of the environment and computer processors which can handle all remote computation. To be fully self reconfigurable these robots need to be autonomous and complete various tasks with minimal or no human interference. This requires precise control algorithms, motion and path planning and even the application of machine learning in some stages.\nTherefore the concept of modular robotics has tremendous research potential and a lot more diverse applications. By showing us new ways robots can interact with the real world and adapt to different conditions, they are slowly evolving and changing our idea of robotics. This very well is just the beginning in the world of Modular Self Reconfigurable Robots.\nReferences:  Modular robotic systems: Methods and algorithms for abstraction, planning, control, and synchronization by Hossein Ahmadzadeh et.al. - https://www.sciencedirect.com/science/article/pii/S0004370215000260 Modular Self-Reconfigurable Robotic Systems: A Survey on Hardware Architectures by S. Sankhar Reddy Chennareddy et. al. - https://www.hindawi.com/journals/jr/2017/5013532/ Current trends in reconfigurable modular robots design by Alberto Brunete et.al. - https://journals.sagepub.com/doi/full/10.1177/172988141771045  ",
    "ref": "/blog/blog/mssr/"
  },{
    "title": "About",
    "date": "",
    "description": "",
    "body": "The Electronics and Robotics Club (ERC) of BITS Goa is a diverse group of students with interests ranging from electronics to machine learning to mechanical design. Over the years, we have evolved into a platform to learn and experiment with various aspects of science and engineering and to apply them in robotics.\nThe club includes people who are passionate about the latest technologies and engineering techniques. This blog is an attempt to share their experience with others in specific fields and also to give people a glimpse of trending areas in robotics research. The posts are written by members with a particular interest in the relevant subject matter and may even correspond to a project that the club has worked upon.\nWe are open to anyone with a general interest in engineering and who wants to explore robotics, so feel free to get in touch with us.\nTo know more about us visit our website, and for learning resources to get started on robotics check out our handbook\n",
    "ref": "/blog/about/"
  },{
    "title": "Kontakt",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/contact/"
  }]
